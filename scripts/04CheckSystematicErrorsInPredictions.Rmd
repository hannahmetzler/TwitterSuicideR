---
title: "Examine misclassified tweets for systematic errors"
author: "Hannah Metzler"
date: "11 Oct 2021"
output: 
  pdf_document:
    df_print: kable
    keep_tex: true
url_colour: blue
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
options(scipen=99)

#libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(ggpubr)
```

# Results with the final training set and predictions with the final model

Predictions were made with BERT fine tuned with LR = 1e5, 10 epochs, seed 1. This model is called BERT 2 in the paper. The BERT 2 model correctly classified 74% of tweets in the test set. F1-scores lay between 70% to 85% for the different categories of interest (see Table 3 and Figure 1a in the paper), with the exception of the suicidal ideation & attempts category with an F1-score of 55%. More specifically, recall for suicidal ideation & attempt was relatively low (49%), indicating difficulties in detecting all such tweets, whereas precision was higher with 62%. Performances were particularly good (>84%) for prevention tweets, and also quite high for tweets about suicide cases (>75%). For coping tweets, BERT 2 achieved very high precision (82%), but lower recall (67%), which resembles the pattern observed for suicidal tweets. Performances for awareness tweets were around 70%. 

```{r}
#load dataset created in script 01b
load('../reliability_datasets/round4_150x5_suicide_human&model_BERT_finetuned.RData')


#format and order variables and labels, so they are the same as with the previous training dataset
d = d4 %>% 
  rename(true_label = main_category.hannah, 
         prediction = main_category.BERT) %>% 
  #labels as sentence case (big first letter)
  mutate(true_label = recode(true_label,  "suicidality" = "Suicidality", "coping" = "Coping", 
    "awareness" = "Awareness", "prevention" = "Prevention", 
    "werther" = "Werther", "irrelevant" = "Irrelevant"), 
    prediction = recode(prediction,  "suicidality" = "Suicidality", "coping" = "Coping", 
    "awareness" = "Awareness", "prevention" = "Prevention", 
    "werther" = "Werther", "irrelevant" = "Irrelevant"))

```

## Confusion Matrix

```{r, fig.width = 4, fig.height=2}
#order of categories for plots
ord_mcategory= rev(c( "Suicidality", "Coping", "Awareness", "Prevention", "Werther", "Irrelevant"))

main_confusion_matrix = as.data.frame(round(
  prop.table(caret::confusionMatrix(d$prediction, d$true_label)$table, margin = 2), 2)) %>% 
    #category order
  mutate(Prediction = factor(Prediction, levels = ord_mcategory, labels = ord_mcategory), 
         Reference = factor(Reference, levels = ord_mcategory, labels = ord_mcategory))%>% 
  #rename some labels for the paper figure
   mutate(Prediction = recode(Prediction, "Suicidality" = "Suicidal", "Werther" = "Suicide cases"), 
          Reference = recode(Reference, "Suicidality" = "Suicidal", "Werther" = "Suicide cases"))

# fontsize = 5 # for png figure for JMIR (max width = 1200 pixels)
fontsize = 5

plotpercent = ggplot(main_confusion_matrix, aes(Prediction, Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = scales::percent(Freq)), size =fontsize-3) + #include only for png figure , size=fontsize-3
  scale_fill_gradient(low = "white", high = "#3575b5") +
  labs(x = "Prediction", y = "True Label", fill = "Percent") + theme_bw()+
  theme(plot.title = element_text(size = fontsize, hjust = 0.5,
                                  margin = margin(20, 0, 20, 0)),
        legend.title = element_text(size = fontsize), #margin = margin(0, 20, 10, 0)),
        axis.title.x = element_text( size = fontsize), #margin = margin(20, 20, 20, 20),
        axis.title.y = element_text(size = fontsize), #margin = margin(0, 20, 0, 10),
        axis.text=element_text(size=fontsize),
        axis.text.x = element_text(angle = 45,  hjust=1),
        legend.position = "none")
#count of tweets
main_confusion_matrix_count = as.data.frame(round(
 caret::confusionMatrix(d$prediction, d$true_label)$table, 2)) %>% 
    #category order
  mutate(Prediction = factor(Prediction, levels = ord_mcategory, labels = ord_mcategory), 
         Reference = factor(Reference, levels = ord_mcategory, labels = ord_mcategory))%>% 
  #rename some labels for the paper figure
   mutate(Prediction = recode(Prediction, "Suicidality" = "Suicidal", "Werther" = "Suicide cases"), 
          Reference = recode(Reference, "Suicidality" = "Suicidal", "Werther" = "Suicide cases"))
plot_count = ggplot(main_confusion_matrix_count, aes(Prediction, Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = (Freq)), size = fontsize-3) + #include only for png figure , size=fontsize-3
  scale_fill_gradient(low = "white", high = "#3575b5") +
  labs(x = "Prediction", y = "", fill = "Percent") + theme_bw()+
  theme(plot.title = element_text(size = fontsize, hjust = 0.5, #+11
                                  margin = margin(20, 0, 20, 0)),
        legend.title = element_text(size = fontsize, margin = margin(0, 20, 10, 0)),
        axis.title.x = element_text( size = fontsize), #margin = margin(20, 20, 20, 20),
        axis.title.y = element_text(size = fontsize), #margin = margin(0, 20, 0, 10), 
        axis.text=element_text(size=fontsize), 
        axis.text.x = element_text(angle = 45,  hjust=1), 
        legend.position = "none")
ggarrange(plotpercent, plot_count,  common.legend = TRUE, legend = "none", labels = c("A", "B"), font.label = list(size = 8))
# ggsave('../figures/Main_category_confusion_matrix.png', width=4, height=1.8, dpi=300, units = 'in', device = 'png')

```

Total number per true category: 

```{r}
d %>%
  group_by(true_label) %>% 
  summarise(n=n())
```


In general, misclassifications as irrelevant are less severe than other confusions, because we prioritize precision (correct predictions for relevant classes) over recall (detecting all true examples). There are two major cases of misclassification between relevant categories: coping tweets get misclassified as suicidal and vice versa, and awareness tweets get misclassified as suicide cases or prevention tweets. The suicidal vs. coping confusion results from.


# Suicidal ideation and attempts (Suicidality)


```{r}
x = d %>% 
  filter(true_label=="Suicidality")
# as.data.frame(xtabs(~prediction, x))%>% 
#   arrange(desc(Freq))
```

## Check Suicidality misclassification as Coping

```{r}
p=as.data.frame(x%>% 
  filter(prediction=="Coping"))
# #print the text only: cannot be printed to PDF with matrix because of emojis in some tweets, at least not with LATEX. Look at the tweets within R. 
p %>%
  mutate(no = 1:n()) %>%
  relocate(no, .before=text) %>%
  select(no, text)

#print the labels only
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text) %>% 
  select(no, category.hannah, category.thomas, prediction)
```

**Of 13 misclassifications of suicidal as coping tweets:**

* 4 also classified as coping by the second coder, in 3 of those cases, the model and coder 2 are actually right. 
* The model might also be right in no 2, 4
* Only 2 are clearly suicidal (no 6, 10), all others are ambiguous. 
* 8 are formulated in the past tense, meaning the suicidal phase could already be over, and the model could be right
* Model right: 5 times for sure.   

## Check Suicidality misclassifications to see how large the confusion with non serious tweets is

This section is reported in the paper. How many predicted labels per category were labelled as not serious by Coder 1? They should all belong to the category irrelevant, but are quite often assigned to the suicidal category. 

How are non-serious tweets distributed across the categories? Mostly irrelevant, but a lot of suicidality. 

```{r}
round(prop.table(xtabs(~notserious_unclear.hannah+prediction, d), margin=1),2)
```

How many tweets per predicted category are not serious or unclear? 

```{r}
round(prop.table(xtabs(~notserious_unclear.hannah+prediction, d), margin=2),2)
```
Absolute values
```{r}
xtabs(~notserious_unclear.hannah+prediction, d)
```

Total n per predicted category
```{r}
as.data.frame(d %>% 
  group_by(prediction) %>% 
  summarise(n=n()))
```
Total n serious vs. not serious
```{r}
as.data.frame(d %>% 
  group_by(notserious_unclear.hannah) %>% 
  summarise(n=n()))
```

Of tweets with the predicted label suicidality, look at those that are not serious or unclear :

```{r}
p = as.data.frame(d %>% 
  filter(prediction=="Suicidality") %>% 
  filter(notserious_unclear.hannah=="1"))
#print the text only
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)

#print the labels only
plabels = p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text) %>% 
  select(no, prediction, category.hannah, notes.hannah)
plabels
```

Which kind of non-serious tweet are these 13 misclassifications?

```{r}
plabels$notes.hannah[4]= "not suicidal"
#11 also seems more like an exaggeration or sarcasm than unclear if serious
plabels$notes.hannah[11]= "sarcasm"
#6 is a comment about weed not making suicidal, this is not actually sarcasm
plabels$notes.hannah[6]= "not suicidal"
xtabs(~notes.hannah, plabels)
```



# Coping

True Coping tweets are misclassified mostly as suicidality. 

```{r}
x = d %>% 
  filter(true_label=="Coping")
# as.data.frame(xtabs(~prediction, x))%>% 
    # arrange(desc(Freq))
```

## Check Coping misclassification as suicidality

```{r, tab.width=0.7}
p = as.data.frame(x%>% 
  filter(prediction=="Suicidality") %>% 
  select(-c(true_label, prediction)))
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text)
```
1 in the past
2 expresses sadness - model error
3 about the past - model error
4 is really ambiguous
5 is about the past - model error
6 is about the past - model error
7 expresses suicidal thoughts, but less than before - model error
8 expresses suicidal thougths but determination not to act - model error
9 ambiguous
10 model error
11 model error, joke
12 model error
13 model error
14 model error, ambiguous
15 ambiguous
16 very ambiguous, coping through medication only
17 past, model error

* Coder 2 disagrees with coping label of coder 1 in 6 out of 17 cases. 
* 5 tweets are about the past, these are hard to get for the model
* 12 are actual errors by the model
* Around 6 are quite ambiguous, understandable errors. 


# Awareness 


```{r}
x = d %>% 
  filter(true_label=="Awareness")
x
# as.data.frame(xtabs(~prediction, x))%>% 
#   arrange(desc(Freq))
```

## Check Awareness misclassification as prevention

```{r, tab.width=0.7}
p = as.data.frame(x%>%
  filter(prediction=="Prevention") %>% 
  select(-c(true_label, prediction)))
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)

p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text)
```

**Of 7 misclassifications as prevention:**

* 4 also classified as prevention by coder 2, all of them are actually prevention, not awareness, i.e. errors by coder 1. (No 2, 5, 6, 7)
* 3 others are ambiguous, not enough information to be clear. 
    
# Prevention 


```{r}
x = d %>% 
  filter(true_label=="Prevention")

# as.data.frame(xtabs(~prediction, x))%>% 
#   arrange(desc(Freq))
```

## Check Prevention misclassification as Suicide other

```{r, tab.width=0.7}
p = as.data.frame(x%>%
  filter(prediction=="Prevention") %>% 
  select(-c(true_label, prediction)))
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)

p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text)
```

**Of 16 misclassifications:**

* no 2, 7 ambiguous
* errors: 1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 14, 15
* model right: 9


\newpage

# Suicide cases 

```{r}
x = d %>% 
  filter(true_label=="Werther")
```

## Check misclassifications as awareness

```{r, tab.width=0.7}
p = as.data.frame(x%>%
  filter(prediction=="Awareness") %>% 
  select(-c(true_label, prediction)))
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)

p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text)
```
* Error: 1, 3, 4, 6, 7, 11, 12, 13, 14, 15, 17
* model right: 2, 9, 
* Ambiguous: 5, 8, 10, 16


# Suicide Other

True Suicide_other tweets are misclassified mostly as irrelevant and Werther, and a bit as awareness and suicidality. 

```{r}
d1 = d
d1$prediction = factor(d1$prediction, levels = c(levels(d$prediction), "bereaved_coping" ,  "bereaved_negative",   "news_coping",    "news_suicidality", "suicide_other",  "off-topic"))

matrix12 = as.data.frame(round(prop.table(table(d1$category.hannah, d1$prediction), margin = 1), 2)[,c(1:6)])
  
ggplot(matrix12, aes(Var2, Var1, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = scales::percent(Freq))) +
  scale_fill_gradient(low = "white", high = "#3575b5") +
  labs(x = "Prediction", y = "True Label", fill = "Percent") + theme_bw()+
  theme(plot.title = element_text(size = 25, hjust = 0.5, 
                                  margin = margin(20, 0, 20, 0)),
        legend.title = element_text(size = 14, margin = margin(0, 20, 10, 0)),
        axis.title.x = element_text( size = 18), #margin = margin(20, 20, 20, 20),
        axis.title.y = element_text(size = 18), #margin = margin(0, 20, 0, 10), 
        axis.text=element_text(size=14), 
        axis.text.x = element_text(angle = 45,  hjust=1))
```

* Bereaved tweets, which necessarily contain a suicide case, were often misclassified as Werther. Also often classified as coping, so at least the personal story aspect was recognized. 
* All tweets about lives saved were misclassified as awareness. 
* News coping tweets were misclassified as prevention. 


## Check Suicide Other misclassification as irrelevant

```{r}
x = d1 %>% 
  filter(category.hannah=="suicide_other")
```

```{r, tab.width=0.7}
p = as.data.frame(x%>% 
  filter(prediction=="Irrelevant") %>% 
  select(-c(true_label, prediction)))
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)

p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text)
```

<!-- Not sure which 14 tweets these were -->
<!-- **Of 14 misclassifications:** -->

<!-- * 1 tweet: Nr 1: model is right - corrected in training set 20201126 -->
<!-- * 8 mistakes are understandable: Nr 2, 3, 5 (could be a joke), 6, 7, 8 (could be a joke), 11, 13 (could be a joke) -->
<!-- * 5 times the model is wrong because the tweet is clearly about suicide: Nr 4, 9, 10 (model missed the murder-suicide), 12, 14 -->
<!-- # 3 could be jokes 5, 8, 13 -->


## Check Suicide Other misclassification as Werther

```{r, tab.width=0.7}
p = as.data.frame(x%>% 
  filter(prediction=="Werther") %>% 
  select(-c(true_label, prediction)))
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text)
```

**Of 15 misclassifications:**

* 1 Model quite right: 13 might also be a Werther tweet but is actually fiction or joke, added second lable in training set 20202611
* 5 Model wrong: 1, 3, 9, 11 (missed the word not), 14
* 3 Error understandable: 4 (corrected in training set 221126 to news_suicidality), 5 (fiction), 10 (murder and suicide separated in sentence)
* 6 Errors that are because the model missed murder-suicide or homicide as clear criterion for suicide_other: 2, 6, 7, 8, 12, 15

## Check Suicide Other misclassification as Awareness

Result: misclassifications as awareness are mostly personal opinions, that should be classified as suicide_other. 

```{r, tab.width=0.7}
p = as.data.frame(x%>% 
  filter(prediction=="Awareness") %>% 
  select(-c(true_label, prediction)))
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text)
```
**7 misclassifications:**

* 1 understandable: could actually be awareness (Nr 4), added double label in 20202611
* 6 Model wrong: the rest are personal opinions

## Check Suicide Other misclassification as Suicidality


```{r, tab.width=0.7}
p = as.data.frame(x%>% 
  filter(prediction=="Suicidality") %>% 
  select(-c(true_label, prediction)))
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(no, text)
p %>% 
  mutate(no = 1:n()) %>% 
  relocate(no, .before=text) %>% 
  select(-text)
```

6 misclassifications

* 4 Understandable: 1, 3 (fiction), 4 (irony), 5 (exaggeration)
* 2 Wrong: 2, 6 

