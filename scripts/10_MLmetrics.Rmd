---
title: "ML metrics and CIs"
author: "Hannah Metzler"
date: "16/11/2021"
output: 
  pdf_document:
    df_print: kable
    keep_tex: true
url_colour: blue
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
options(scipen=99)
library(dplyr)
library(caret)
# library(epiR)
library(boot)

#function for calculating bootstrapped CIs: 
# intro to boot package: https://www.geeksforgeeks.org/bootstrap-confidence-interval-with-r-programming/
#function to calculate the statistics precision, recall, F1, accuracy for each of the 6 categories
mlperf6.fun = function(data, idx, class){
  df <- data[idx, ]
  #out of the confusion Matrix, we only select the byClass metrics, here for only one class at a time
  perf_class = confusionMatrix(df$true_label, df$predicted_label)$byClass[paste("Class:", class), 
                                                                          c("Precision", "Recall", "F1")]
  return(perf_class)
}

#accuracy per class function
acc.fun= function(data, idx, class){
  df <- data[idx, ]
  Acc = df %>% 
    summarise(df, sum(predicted_label==true_label)/n())
  return(Acc)
}
data = test_6cat
data = filter(data, true_label==iclass)
bootstrap = boot(data, acc.fun, class=iclass, R=1000)
#this accuracy function still throws an error, check

#order of levels, function: 
order_cat = c("suicidality", 'coping', "awareness", "prevention", "werther", "irrelevant" )
order6cat = function(df){
    df = df %>% 
      mutate(true_label = factor(true_label, labels=order_cat, levels=order_cat), 
      predicted_label = factor(predicted_label, labels=order_cat, levels=order_cat))
    return(df)
}

#function to calculate the statistics for each of the 2 about suicide categories
mlperf2.fun = function(data, idx, class){
  df <- data[idx, ]
   #out of the confusion Matrix, we only select the byClass metrics, here for only one class at a time
  perf_class = confusionMatrix(df$true_label, df$predicted_label, positive=iclass)$byClass[c("Precision", "Recall", "F1")]

  return(perf_class)
}
```



# Caculate per class metrics that were missing in Hubert's dataset, for different models

* Data from Hubert's excel file (Twitter Results Hubert), sheet AboutSuicide/Not column J

## About suicide, Bert unbalanced dataset, recommended settings, 3e-5, 5

```{r setup, include=FALSE}
labels <- c("aboutsuicide", "notsuicide")
truth <- factor(rep(labels, times = c(455+22, 58+90)),
                levels = (labels))
pred <- factor( c(rep(labels, times = c(455, 22)),
                  rep(labels, times = c(58, 90))),
                levels = (labels))
xtab <- table(pred, truth)
```

* this fits Hubert's stats here: https://imgur.com/AwBYNoS
* Accuracy = average accuracy across both classes
* the other metrics (in last paragraph) are for the positive class
* positive pred value = precision
* sensitivity = recall

```{r setup, include=FALSE}
average = caret::confusionMatrix(xtab, positive = "notsuicide")$overall[c("Accuracy", "AccuracyLower", "AccuracyUpper")]
  byclass = round(data.frame(
  aboutsuicide = caret::confusionMatrix(xtab, positive = "aboutsuicide")$byClass[c("Precision", "Recall", "F1")],
  notsuicide = caret::confusionMatrix(xtab, positive = "notsuicide")$byClass[c("Precision", "Recall", "F1")]), 2)
# write.csv(file = "output/ML_byclass_performance.csv", t(byclass))
```

## About suicide,BERT unbalanced (1e-5, 10, 1) - final model reported in the paper

```{r setup, include=FALSE}
####
labels <- c("aboutsuicide", "notsuicide")
truth <- factor(rep(labels, times = c(452+25, 49+99)),
                levels = (labels))
pred <- factor( c(rep(labels, times = c(452, 25)),
                  rep(labels, times = c(49, 99))),
  levels = (labels))
xtab <- table(pred, truth)
average = caret::confusionMatrix(xtab, positive = "notsuicide")$overall[c("Accuracy", "AccuracyLower", "AccuracyUpper")]
byclass = round(data.frame(
  aboutsuicide = caret::confusionMatrix(xtab, positive = "aboutsuicide")$byClass[c("Precision", "Recall", "F1")],
  notsuicide = caret::confusionMatrix(xtab, positive = "notsuicide")$byClass[c("Precision", "Recall", "F1")]), 2)
# write.csv(file = "output/ML_byclass_performance_about_suicide.csv", t(byclass))
```

## XLNet

```{r setup, include=FALSE}
labels <- c("aboutsuicide", "notsuicide")
truth <- factor(rep(labels, times = c(448+29,46+102)),
                levels = (labels))
pred <- factor( c(rep(labels, times = c(448,	29)),
                  rep(labels, times = c(46,	102))),
                levels = (labels))
xtab <- table(pred, truth)
average = caret::confusionMatrix(xtab, positive = "notsuicide")$overall[c("Accuracy", "AccuracyLower", "AccuracyUpper")]
byclass = round(data.frame(
  aboutsuicide = caret::confusionMatrix(xtab, positive = "aboutsuicide")$byClass[c("Precision", "Recall", "F1")],
  notsuicide = caret::confusionMatrix(xtab, positive = "notsuicide")$byClass[c("Precision", "Recall", "F1")]), 2)
# write.csv(file = "output/ML_byclass_performance_about_suicide.csv", t(byclass))
```

## TFIDF

```{r setup, include=FALSE}
labels <- c("aboutsuicide", "notsuicide")
truth <- factor(rep(labels, times = c(417+60,60+88)),
                levels = (labels))
pred <- factor( c(rep(labels, times = c(417,60)),
                  rep(labels, times = c(60,88))),
                levels = (labels))
xtab <- table(pred, truth)
average = caret::confusionMatrix(xtab, positive = "notsuicide")$overall[c("Accuracy", "AccuracyLower", "AccuracyUpper")]
byclass = round(data.frame(
  aboutsuicide = caret::confusionMatrix(xtab, positive = "aboutsuicide")$byClass[c("Precision", "Recall", "F1")],
  notsuicide = caret::confusionMatrix(xtab, positive = "notsuicide")$byClass[c("Precision", "Recall", "F1")]), 2)
# write.csv(file = "output/ML_byclass_performance_about_suicide.csv", t(byclass))
```


