---
title: "Tweets per Category in the ML prediction dataset (BERT) based on Crimson Hexagon query with retweets"
author: "Hannah Metzler"
date: "4th October 2021"
output: 
  pdf_document:
    df_print: kable
    keep_tex: true
url_colour: blue
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, results=T)
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
options(scipen=99)
library(dplyr)
library(ggplot2)
# library(viridis)
library(tidyr)
library(lubridate)
#for wordclouds: 
library(beepr) # to beep after a long analysis is finished
library(wordcloud)
library(tidytext) #for making word list with frequencies
library(stringr)
```


```{r data, include=FALSE}
#load data (origin: scripts folder, where this .Rmd file is)
load("../data_tweet_volumes/Suicide_tweets_daily_volume_per_maincategory_withRTs.R")
# load("../data_tweet_volumes/Suicide_tweets_daily_volume_aboutsuicide.R")

#or source script (but this takes a couple of minutes)
# source('06BuildpredictionTimelineDataset.R')

#order of categories for plots
ord_mcategory= c( "suicidality", "coping", "awareness", "prevention", "werther", "irrelevant")

#rename main_category_prediction to prediction, but keep in mind about_suicide_n is also calculated based on model prediction (with the categories yes/no)
dcatperday = dcatperday %>% 
  rename(prediction = main_category_prediction) %>% 
  #category order
  mutate(prediction = factor(prediction, levels = ord_mcategory, labels = ord_mcategory))

summary(dcatperday)

# figure settings
s = 17
#assign colours to categories:
catcols = c("#332288", "#88CCEE", "#44AA99",  '#DDCC77', "#117733", '#999933')
mycols = c("awareness"= catcols[1], "werther"=catcols[2], "prevention"=catcols[3], "suicidality"=catcols[4], "coping"=catcols[5], "irrelevant"=catcols[6])
mycols2 = c("Awareness"= catcols[1], "Suicide cases"=catcols[2], "Prevention"=catcols[3], "Suicidal ideation & attempts"=catcols[4], "Coping"=catcols[5], "Irrelevant"=catcols[6])


#plot data with labels as in paper: 
dpaper = dcatperday %>% 
  #category order
  mutate(prediction = recode(prediction, suicidality = "Suicidal ideation & attempts", coping = "Coping", 
                             awareness = "Awareness", prevention = "Prevention", 
                             werther = "Suicide cases", irrelevant = "Irrelevant")) %>% 
   filter(year %in% c(2016, 2017, 2018)) %>% 
  #daily percentage
    group_by(date, prediction) %>%
    mutate (n = sum(ntweets)) %>%
    mutate(percent = round(n/ndaytotal*100,2), 
           year = as.factor(year))
```



# Dataset description

We initially downloaded tweets from crimson hexagon on April 15th 2020, including all tweets containing a suicide-related term from January 1st 2013 to April 14th 2020. We excluded terms that clearly do not refer to suicide as the act of killing oneself. The query on Crimson Hexagon was: 

country:USA AND language:en
AND (suicide OR suicidal OR "killed himself" OR "killed herself" OR "kill himself" OR "kill herself" OR "hung himself" OR "hung herself" OR "took his life" OR "took her life" OR "take his life" OR "take her life" OR "end his own life" OR "end her own life" OR "ended his own life" OR "ended her own life" OR "end his life" OR "end her life" OR "ended his life" OR "ended her life" OR "ends his life" OR "ends her life") 
AND NOT ("suicide squad" OR suicidechrist OR suicidegirl* OR suicideboy* OR suicideleopard OR suicidexjockey* OR "suicidal grind" OR bomber OR squad OR epstein OR Trump OR clinton* OR Hillary OR Biden OR sanders OR “political suicide”) 

We rehydrated these tweets a first time in April 2020, the resulting dataset included around 13.428 million tweets, of which only 9.584 million had unique ids. The machine learning models were trained on a sample of 3200 manually annotated tweets from this sample. Because our initial try to multiply each tweet by its number of retweet did not accurately mirror the daily volumes directly downloaded via Brandwatch, we set up another query with retweets, and rehydrated this tweets. This document describes this final dataset, which was used in the follow-up timeseries analysis. 

prediction were made with the BERT base model, fine-tuned (more epochs, smaller learning rate, check with Hubert for details). 

* First included date: 2016-01-01
* Last included date: 2021-04-13

# Sample descriptives 
### Total sample size

```{r}
dcatperday %>%
  group_by(date) %>% #only first line of 6 categories (ndaytotal is the same on each line)
  slice(1) %>% 
  ungroup() %>% 
  summarise(mio_tweets = sum(ndaytotal)/1000000)
```

Total sample size 2016-2018 for paper: 

```{r}
dcatperday %>%
  filter(year %in% c(2016, 2017, 2018)) %>% 
  group_by(date) %>% #only first line of 6 categories (ndaytotal is the same on each line)
  slice(1) %>% 
  ungroup() %>% 
  summarise(mio_tweets = sum(ndaytotal))
```


### Sample per year 
```{r}
dcatperday %>%
  group_by(date) %>% #only first line of 6 categories (ndaytotal is the same on each line)
  slice(1) %>% 
  group_by(year) %>% 
  summarise(mio_tweets = sum(ndaytotal)/1000000) %>% 
  #2020 is not a full year 
  filter(year!="2021")
```

### Total number per category for paper (2016-2018)

```{r}
prop.pred.grand = as.data.frame(
  dcatperday %>%
    filter(year %in% c(2016, 2017, 2018)) %>% 
    group_by(prediction) %>%
    summarise (n = sum(ntweets)) %>%
    mutate(freq = round(n/ sum(n)*100,2)))

#CI
binomci <- function(prop.pred.grand){
  for (i in seq(1:nrow(prop.pred.grand))){
    ci =with(prop.pred.grand, binom.test(x = n[i], n =sum(n)))$conf.int
    prop.pred.grand$cilow[i] = ci[1]
    prop.pred.grand$cihigh[i] = ci[2]
  }
 return(prop.pred.grand)
}
prop.pred.grand = binomci(prop.pred.grand)

#adjust by the recall of the model: divide by the recall (because I want to increase the number if the recall is low)
prop.pred.grand$recall_bert = c(0.45, 0.69, 0.70, 0.89, 0.77, NA)
prop.pred.grand = prop.pred.grand %>% 
  mutate(n_adjusted = n/recall_bert) %>% 
  mutate(freq_adjusted = round(n_adjusted/sum(prop.pred.grand$n)*100,2))
  #correct n for category irrelevant (because dividing by recall does not work there)
prop.pred.grand[prop.pred.grand$prediction=="irrelevant", "freq_adjusted"] = 100-sum(prop.pred.grand$freq_adjusted, na.rm = T)
# write.csv(file="../results/estimated_freq_per_category_retweet_dataset1.csv", prop.pred.grand)
prop.pred.grand
```

```{r}
ggplot(prop.pred.grand, aes(x=prediction, y = freq, fill=prediction)) + #dataset and variables to plot
  geom_bar(stat="identity")+
  geom_errorbar(aes(x=prediction, ymin = freq-cilow, ymax=freq+cihigh), size=1, width=0.5)+
  ggtitle("Tweets per predicted category")+
  labs(y="Proportion", x="") + #axes and title labels
  scale_fill_manual(values=c("#332288", "#88CCEE", "#44AA99", "#117733", '#999933', '#DDCC77', '#CC6677', "#882255", "#332288", "#88CCEE", "#44AA99", "#AA4499", "#999999",  "#444444"))+
  theme_bw() +  
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position="none", text=element_text(size=s))
```
Not adjusted by model recall

### Standard deviation, min and max across daily proportions per category

```{r}
prop.pred.daily = dcatperday %>%
  filter(year %in% c(2016, 2017, 2018)) %>% 
  #collapse across tweet type (original vs retweet)
  group_by(date, prediction) %>% 
  mutate(ncat = sum(ntweets)) %>% 
  slice(1) %>% #drop 2nd line
  group_by(prediction) %>% 
  
  #daily percentage
  mutate(freq = ncat/ndaytotal*100) %>% 
  #mean and sd across all days
  group_by(prediction) %>% 
  summarise(mean = round(mean(freq),2),
            sd = round(sd(freq),2), 
            min = round(min(freq), 2), 
            max = round(max(freq), 2)) %>% 
  #adjust for model recall
  mutate(mean_adjusted = round(mean/prop.pred.grand$recall_bert, 2))
#correct n for category irrelevant (because dividing by recall does not work there)
prop.pred.daily[prop.pred.daily$prediction=="irrelevant", "mean_adjusted"] = 100-sum(prop.pred.daily$mean_adjusted, na.rm = T)
prop.pred.daily
#   write.csv(file="../results/estimated_freq_per_category_retweet_dataset_sd_min_max.csv", prop.pred.daily)

ggplot(prop.pred.daily, aes(x=prediction, y = mean, fill=prediction)) + #dataset and variables to plot
  geom_bar(stat="identity")+
  # geom_errorbar(aes(x=prediction, ymin = mean_adjusted-cilow, ymax=mean_adjusted+cihigh), size=1, width=0.5)+
  ggtitle("Average across daily percentages")+
  labs(y="Proportion", x="") + #axes and title labels
  scale_fill_manual(values=c("#332288", "#88CCEE", "#44AA99", "#117733", '#999933', '#DDCC77', '#CC6677', "#882255", "#332288", "#88CCEE", "#44AA99", "#AA4499", "#999999",  "#444444"))+
  theme_bw() +  
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position="none", text=element_text(size=s))

```



## Total number of tweets per category by year

* Irrelevant increase over the years may simply reflect increase in tweets overall. Irrelevant includes tweets about suicide not belonging in any other category, and off-topic tweet (metaphors, jokes, euthansia, bombings,...). 
* the total volume (sum of all categories) is shown in black

```{r}
dcatperyear = dcatperday %>% 
  group_by(year, prediction) %>% 
  summarise(ntweets = sum(ntweets), 
            pr = sum(ntweets)/sum(ndaytotal)) %>% 
  group_by(year) %>% 
       mutate(ntotal = sum(ntweets)) %>% 
  #2021 is not a full year 
  filter(year!="2021") %>% 
  #million tweets
  mutate(mio_tweets = ntweets/1000000,
         mio_total = ntotal/1000000)

#plot per year
ggplot(data=dcatperyear, aes(x=year, y=ntweets))+
  geom_point(aes(colour = prediction))+ geom_line(aes(colour = prediction))+
  geom_point(aes(x=year, y=ntotal))+  geom_line(aes(x=year, y=ntotal))+
  #show total tweets with suicide terms per year (sum of all categories)
  # geom_line(data=dtotperyear, aes(x=year, y=ndaytotal))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  scale_x_continuous(n.breaks=7)
```

```{r}
ggplot(data=filter(dcatperyear, prediction!="irrelevant"), aes(x=year, y=mio_tweets, colour = prediction))+
  geom_point()+ geom_line()+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  scale_x_continuous(n.breaks=7)+
  ggtitle(" Same plot without the category Irrelevant")
  
```

* also shows the general increase for Werther tweets. 
* the increase exists since 2017, but is much smaller, for coping and suicidality tweets

The percentage of tweets per category decreased for irrelevant, and increased for the other categories. 

```{r}
#plot per year
ggplot(data=dcatperyear, aes(x=year, y=pr))+
  geom_point(aes(colour = prediction))+ geom_line(aes(colour = prediction))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  scale_x_continuous(n.breaks=7)+ggtitle("Percentage of tweets per category in total number of tweets")
```


# Monthly pattern? Total number of tweets per month (averaged across 7 years)

```{r}
dcatpermonth = dcatperday %>% 
  filter(date < as.Date("2020-04-01")) %>% #no full month of data
  group_by(month, prediction) %>% 
  summarise(ntweets = sum(ntweets)) %>% 
  #thousand tweets
  mutate(thousand_tweets = ntweets/1000) %>% ungroup()
#plot per month
ggplot(data=dcatpermonth, aes(x=month, y=ntweets, colour = prediction))+
  geom_point()+ geom_line(aes(group=prediction))+
  theme_bw()+
  scale_colour_manual(values=mycols)

```

* Open question: Are the peaks in certain categories driven by outlier months in some years, or general patterns?
    - September: world suicide prevention day is likely a general pattern
    - Werther/Prevention ups and downs are probably influenced by celebrity suicides in one or some of the years.

# Monthly timeline across years (January 2015 to March 2020)

```{r}
#monthly timeseries
dtsmonthly = dcatperday %>% 
  #filter(date < as.Date("2020-04-01")) %>% #no full month of data
  group_by(year, month, prediction) %>% 
  summarise(ntweets = sum(ntweets)) %>% 
  #thousand tweets
  mutate(thousand_tweets = ntweets/1000) %>% ungroup() %>% 
#add median per month 
  group_by(month, prediction) %>% 
  mutate(ntweets_median = median(ntweets))

#plot per month
ggplot(data=dtsmonthly)+
  geom_boxplot(aes(x=month, y=ntweets, colour = prediction)) +
  # geom_point(aes(x=month, y=ntweets, colour = prediction))+ 
  geom_line(aes(x=month, y=ntweets_median, group=prediction, colour=prediction))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("Boxplot per year, median as a line")
```


More detailed picture (without irrelevant category):

```{r}
ggplot(data=filter(dtsmonthly, prediction!="irrelevant"))+
  geom_boxplot(aes(x=month, y=ntweets, colour = prediction)) +
  # geom_point(aes(x=month, y=ntweets, colour = prediction))+ 
  geom_line(aes(x=month, y=ntweets_median, group=prediction, colour=prediction))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("Boxplot per year, median as a line (without irrelevant)")
```



```{r}
ggplot(data=filter(dtsmonthly, prediction!="irrelevant"))+
  # geom_violin(aes(x=month, y=ntweets, colour = prediction))+ 
  geom_point(aes(x=month, y=ntweets, colour = prediction))+ 
  geom_line(aes(x=month, y=ntweets_median, group=prediction, colour=prediction))+
  facet_wrap(~prediction)+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  theme(axis.text.x=element_text(angle=60, hjust=1), axis.title.x=element_blank(), legend.position="none")+
  ggtitle("One dot per year, median as a line")
```

* September prevention is systematically higher (world suicide prevention day). Maybe also slightly higher around Christmas. 
* The Werther outlier in June could be Spade and Bourdain in 2018?
* Awareness median also regularly higher in August, lower in June. 


# Weekly pattern? Total number of tweets per weekday (averaged across 7 years)

```{r}
dcatperwday = dcatperday %>% 
  group_by(weekday, prediction) %>% 
  summarise(median_tweets = median(ntweets)) %>% 
  #thousand tweets
  mutate(thousand_tweets = median_tweets/1000) %>% ungroup()
#plot per week day
ggplot(data=dcatperwday, aes(x=weekday, y=median_tweets, colour = prediction))+
  geom_point()+ geom_line(aes(group=prediction))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("Median tweets per weekday")
ggplot(data=filter(dcatperwday, prediction !="irrelevant"), aes(x=weekday, y=median_tweets, colour = prediction))+
  geom_point()+ geom_line(aes(group=prediction))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("Median tweets per weekday (without irrelevant category)")
ggplot(data=filter(dcatperwday, prediction =="coping" | prediction == "suicidality"), aes(x=weekday, y=median_tweets, colour = prediction))+
  geom_point()+ geom_line(aes(group=prediction))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("Median tweets per weekday: suicidality and coping category)")
```


# Daily timeline of daily tweets per category

```{r, fig.width=10, fig.height=12}
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
plot_timeseries = ggplot(data=dpaper, aes(x=date, y=percent, colour = prediction))+
  geom_line()+
  theme_bw()+
  scale_colour_manual(values=mycols2)+
  scale_x_date(breaks = seq(as.Date("2016-01-01"), as.Date("2019-01-01"), by="3 months"), 
               date_labels = "%b %y", minor_breaks = "1 month", expand = c(0.01,0.01))+
  facet_wrap(~prediction, ncol=1)+
  ylab("Percent per predicted category")+
  theme(legend.position="none", text=element_text(size=s+5), axis.title.x=element_blank(), plot.margin = margin(rep(0.5,4), unit="cm"), 
        axis.text.x = element_text(angle = 45, hjust=1))+
  geom_vline(aes(xintercept=as.Date("2017-01-01")), colour = "grey", linetype=3)+
  geom_vline(aes(xintercept=as.Date("2018-01-01")), colour = "grey", linetype = 3)
plot_timeseries
# ggsave('../figures/Daily_timeseries_per_categoryR.pdf',  plot_timeseries, width=10, height=13, dpi=300)
```

  # Idenfication of peaks and associated events

```{r, tweet text preprocessing}
#set date format to english
Sys.setlocale("LC_ALL", 'en_US.UTF-8')

#new predictions file based on Crimson Hexagon query including Retweets, including the text of tweets
dp = read.csv('../full_tweet_set_for_predictions_withRTs_inquery/twitter_14M_with_predictions_withRTs.csv', stringsAsFactors=F, sep='\t', colClasses = "character")

#format variables and select 3 years for paper
dp = dp %>% 
  mutate(main_category = factor(main_category), 
         about_suicide = factor(about_suicide)) %>% 
  #filter the 3 years for the paper
  filter(str_detect(time, "2016") | str_detect(time, "2017")|  str_detect(time, "2018")) %>% 
  #format date
  mutate(date = lubridate::mdy(time, locale = Sys.getlocale("LC_TIME"))) %>% 
  arrange(date)

dp = dp1

#from wordcloud tutorial: https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a
#clean the tweet text
dp1$text = gsub("https\\S*", "", dp1$text) 
dp1$text =gsub("@\\S*", "", dp1$text) 
dp1$text =gsub("amp", "", dp1$text) 
dp1$text =gsub("[\r\n]", "", dp1$text)
dp1$text =gsub("[[:punct:]]", "", dp1$text)
beep(sound = 1, expr = NULL) #make beep when finished

# function for making a wordcloud with this dataset
#make word list with frequencies
wordcloud_suicide = function(dcloud){
  words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
  
  #make wordcloud
  set.seed(1234) # for reproducibility 
  wordcloud::wordcloud(words =words$word, freq = words$n, min.freq = 5,           
                       max.words=100, random.order=FALSE, rot.per=0.35,            
                       colors=brewer.pal(8, "Dark2"))
}
```

## Coping

```{r, coping, fig.width=10, fig.height=4}
(ggplot(data=filter(dpaper, prediction=="Coping"), 
       aes(x=date, y=percent, colour = year))+
  geom_line()+
  theme_bw()+
  facet_wrap(~year, scales = "free_x", ncol=1)+
  scale_x_date(breaks = seq(as.Date("2016-01-01"), as.Date("2019-01-01"), by="1 months"), 
               date_labels = "%b", minor_breaks = "1 month", expand = c(0.01,0.01))+
  scale_colour_manual(values=catcols)+
  ylab("Percent per predicted category")+
  theme(legend.position="none", text=element_text(size=s), axis.title.x=element_blank(), plot.margin = margin(rep(0.5,4), unit="cm")))
```

Peaks above 5% daily tweets for coping stories: 

```{r}
dpaper %>%
 filter(prediction=="Coping")%>%
  filter(percent>20) %>%
  select(prediction, date, percent) %>% 
  group_by(date) %>% slice(1) %>% 
  arrange(date)
```

### Peak on 2017-11-24

```{r, wordcloud 2017-11-24, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "coping"& date=="2017-11-24" | main_category == "coping"& date=="2017-11-25")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words
```
Around 6000 retweets of: 

```{r}
dp %>% 
  filter(main_category == "coping"& date=="2017-11-24") %>% 
  filter(str_detect(text, "weeks")) %>% 
  select(text) %>% slice(1)
```




### Peak on 2018-08-24

```{r, wordcloud 2018-08-24, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "coping"& date=="2018-08-24" | main_category == "coping"& date=="2018-08-25")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words[10:20,]
```

Around 5000 retweets of: 

```{r}
dp %>% 
 filter(main_category == "coping"& date=="2018-08-24" | main_category == "coping"& date=="2018-08-25") %>% 
  filter(str_detect(text, "Marine")) %>% 
  select(text) %>% slice(1)
```




## Suicidal ideation 

```{r, suicidal}
(ggplot(data=filter(dpaper, prediction=="Suicidal ideation & attempts"), 
       aes(x=date, y=percent, colour = year))+
  geom_line()+
  theme_bw()+
  facet_wrap(~year, scales = "free_x", ncol=1)+
  scale_x_date(breaks = seq(as.Date("2016-01-01"), as.Date("2019-01-01"), by="1 months"), 
               date_labels = "%b", minor_breaks = "1 month", expand = c(0.01,0.01))+
  scale_colour_manual(values=catcols)+
  ylab("Percent per predicted category")+
  theme(legend.position="none", text=element_text(size=s), axis.title.x=element_blank(), plot.margin = margin(rep(0.5,4), unit="cm")))
```

Peaks above 5% daily tweets for coping stories: 

```{r}
dpaper %>%
 filter(prediction=="Suicidal ideation & attempts")%>%
  filter(percent>12) %>%
  select(prediction, date, percent) %>% 
  group_by(date) %>% slice(1) %>% 
  arrange(date)
```

### Peak on 2016-01-19

```{r, wordcloud 2016-01-19, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "suicidality"& date=="2016-01-19")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words[10:20,]
```

846 retweets of: 

```{r}
dp %>% 
  filter(main_category == "suicidality"& date=="2016-01-19") %>% 
  filter(str_detect(text, "BITCH")) %>% 
  select(text) %>% slice(1)
```





### Peak on 2018-05-21	

```{r, wordcloud 2018-05-21, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "suicidality"& date=="2018-05-21" | main_category == "suicidality"& date=="2018-05-22")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words[15:20,]
```

Around 4000 retweets of: 

```{r}
dp1 %>% 
  filter(main_category == "suicidality"& date=="2018-05-21") %>% 
    filter(str_detect(text, "jobCareer")) %>% 
  select(text) %>% slice(1)
```

This tweet continued like this, the model did not have the full text: "Man y’all. I’m 21. I have a degree. A full time job(Career). And am about to get a second degree for free. Y’all I’m from the hood. I was born to a 15 year old mother. I battled suicide for 5 years. This was not supposed to be my story. To God be the glory."
It's a false positive, should have been a coping tweet. 



### Peak on 2018-10-12


```{r, wordcloud 2018-10-121, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "suicidality"& date=="2018-10-12" | main_category == "suicidality"& date=="2018-10-13")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words[15:21,]
```

Around 1144 retweets of: "Therapist: any suicidal thoughts? me, has been thinking "I wish I was dead" every 5 minutes since middle school: no, none..."

```{r}
dp1 %>% 
  filter(main_category == "suicidality"& date=="2018-10-12") %>% 
    filter(str_detect(text, "therapist")) %>% 
  select(text) %>% slice(1)
```

## Awareness

```{r}
(ggplot(data=filter(dpaper, prediction=="Awareness"), 
       aes(x=date, y=percent, colour = year))+
  geom_line()+
  theme_bw()+
  facet_wrap(~year, scales = "free_x", ncol=1)+
  scale_x_date(breaks = seq(as.Date("2016-01-01"), as.Date("2019-01-01"), by="1 months"), 
               date_labels = "%b", minor_breaks = "1 month", expand = c(0.01,0.01))+
  scale_colour_manual(values=catcols)+
  ylab("Percent per predicted category")+
  theme(legend.position="none", text=element_text(size=s), axis.title.x=element_blank(), plot.margin = margin(rep(0.5,4), unit="cm")))
```

* Large awareness peaks above 6000 daily tweets:

```{r}
dpaper %>%
  filter(prediction=="Awareness") %>%
  filter(percent>50) %>%
  select(prediction, date, percent) %>% 
  group_by(date) %>% slice(1) %>% 
  arrange(date) %>% 
  select(date, percent)
```
### Peak on 2016-04-22

```{r, wordcloud 2016-04-22, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "awareness"& date=="2016-04-22")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words
```

Around 900 retweets of: 

```{r}
dp %>% 
  filter(main_category == "awareness"& date=="2016-04-22") %>% 
  filter(str_detect(text, "rate")) %>% 
  select(text)
```



### Peak on 2017-02-21

```{r, wordcloud 2017-02-21, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "awareness"& date=="2017-02-21")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words
```

Around 3000 retweets of: 

```{r}
dp %>% 
  filter(main_category == "awareness"& date=="2017-02-21") %>% 
  filter(str_detect(text, "marriage")) %>% 
  select(text)
```



### Peak on 2017-07-29

```{r, wordcloud 2017-07-29, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "awareness"& date=="2017-07-29")
wordcloud_suicide(dcloud)
```
Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words
```

Around 3500 retweets of: 

```{r}
dp1 %>% 
  filter(main_category == "awareness"& date=="2017-07-29") %>% 
  filter(str_detect(text, "sympathy")) %>% 
  select(text) %>% slice(1)
```




### Peak on 2017-11-20

Peak lasts for 2 days

```{r, wordcloud 2017-11-20, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "awareness"& date=="2017-11-20" | main_category == "awareness"& date=="2017-11-21")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words[10:20,]
```

Around 7000 retweets of: 

```{r}
dp %>% 
  filter(main_category == "awareness"& date=="2017-11-20") %>% 
  filter(str_detect(text, "biggest")) %>% 
  select(text) %>% slice(1)
```


### Peak on 2018-08-17	
```{r, wordcloud 2018-08-17, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "awareness"& date=="2018-08-17")
wordcloud_suicide(dcloud)
```

Word frequency: 

```{r}
words <- dcloud %>%
    #toknize
    tidytext::unnest_tokens(output= word, input= text) %>% 
    #calculate frequency
    count(word, sort=TRUE)
words
```

Around 6000 retweets of: 

```{r}
dp1 %>% 
  filter(main_category == "awareness"& date=="2018-08-17") %>% 
  filter(str_detect(text, "boys")) %>% 
  select(text)
```


## Prevention

```{r}
(ggplot(data=filter(dpaper, prediction=="Prevention"), 
       aes(x=date, y=percent, colour = year))+
  geom_line()+
  theme_bw()+
  facet_wrap(~year, scales = "free_x", ncol=1)+
  scale_x_date(breaks = seq(as.Date("2016-01-01"), as.Date("2019-01-01"), by="1 months"), 
               date_labels = "%b", minor_breaks = "1 month", expand = c(0.01,0.01))+
  scale_colour_manual(values=catcols)+
  ylab("Percent per predicted category")+
  theme(legend.position="none", text=element_text(size=s), axis.title.x=element_blank(), plot.margin = margin(rep(0.5,4), unit="cm")))
```

* Large prevention peaks: 
    - Regular peak every september, world suicide prevention day (2017-09-11)
    - Regular peak end of December: Christmas (2018-12-25, 22-25 Dec 2017), (New Years 2018-01-01)

```{r}
dpaper %>%
  filter(prediction=="Prevention") %>%
  filter(percent>65) %>%
  select(prediction, date, percent) %>% 
  group_by(date) %>% slice(1) %>% 
  arrange(date) %>% 
  select(date, percent)
```

Peaks in September of every year: 

 * Usually, there is a peak around September 10-11 for World suicide prevention day. 
 * In 2017, there are additional peaks on Sep 2-3. 

```{r}
dpaper %>%
  filter(prediction=="Prevention" & month=="Sep") %>%
  filter(percent>40) %>%
  select(prediction, date, percent) %>% 
  group_by(date) %>% slice(1) %>% 
 arrange(date) %>% 
  select(date, percent)
```

```{r}
dpaper %>%
  filter(prediction=="Prevention" & month=="Dec" | prediction=="Prevention" & month == "Jan") %>%
  filter(percent>35) %>%
  select(prediction, date, percent) %>% 
  group_by(date) %>% slice(1) %>% 
 arrange(date) %>% 
  select(date, percent)
```
Christmas and New Year: 

 * before and after Christmas (22-26) every year
 * January 1 in 2018, smaller peaks on New Years day in 2017/2019
 * Additionally: 
    - 2017-01-08/09
    - 2017-01-20

Other events: 

* 2016-11-09: Trump election, checked with wordcloud below
* 2017-03-19
* 2018-10-20: many tweets about hotline and eating disorders (wordcloud below)
    - suicide cluster: https://www.newyorker.com/magazine/2021/04/19/a-mysterious-suicide-cluster -->


### Peak on 2016-11-10

```{r, 2016-11-10, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "prevention"& date=="2016-11-10")
wordcloud_suicide(dcloud)
```

Linked to calls to suicide hotline/lifeline after Trumps election

### Peak on 2018-10-20

```{r, 2018-10-20, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "prevention"& date=="2018-10-20")
wordcloud_suicide(dcloud)
```
Eating disorders, addition and the number of the lifeline. 

    
## Suicide case reports

```{r}
ggplotly(ggplot(data=filter(dpaper, prediction=="Suicide cases"), 
       aes(x=date, y=percent, colour = year))+
  geom_line()+
  theme_bw()+
  facet_wrap(~year, scales = "free_x", ncol=1)+
  scale_x_date(breaks = seq(as.Date("2016-01-01"), as.Date("2019-01-01"), by="1 months"), 
               date_labels = "%b", minor_breaks = "1 month", expand = c(0.01,0.01))+
  scale_colour_manual(values=catcols)+
  ylab("Percent per predicted category")+
  theme(legend.position="none", text=element_text(size=s), axis.title.x=element_blank(), plot.margin = margin(rep(0.5,4), unit="cm")))
```

Useful link: List of death by year on [Wikipedia](https://en.wikipedia.org/wiki/Lists_of_deaths_by_year). 
[List of suicides in the 21st century](https://en.wikipedia.org/wiki/List_of_suicides_in_the_21st_century)

* 5  large Werther peaks (see below for wordclouds): 
 - 2016-02-05: Mirra David suicide on February 4, 2016. [Wikipedia](https://en.wikipedia.org/wiki/Dave_Mirra), 
 - 2017-09-08: a women remembering her husband's suicide, many retweets
 - 2017-10-05: Las Vegas shooting, a reply to a tweet by Trump was retweeted a lot, about the shooter killing himself
 - 2017-11-04: Brad Bufada on November 3, from Wikipedia suicide list, [Article](https://variety.com/2017/tv/news/brad-bufanda-dead-dies-veronica-mars-1202606441/)
 - 2018-02-02

```{r}
dpaper %>%
  filter(prediction=="Suicide cases") %>%
  filter(percent>50) %>%
  select(prediction, date, percent) %>% 
  group_by(date) %>% slice(1) %>% 
  arrange(date)
```
### Peak on 2016-02-05

```{r, wordcloud 2016-02-05, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2016-02-05")
wordcloud_suicide(dcloud)
```
### Peak on 2017-09-08

```{r, wordcloud  2017-09-08, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date==" 2017-09-08")
wordcloud_suicide(dcloud)
```

```{r}
dp1 %>% 
  filter(main_category == "werther"& date==" 2017-09-08") %>% 
  select(text) %>% slice(1)
```


These are essentially a lot of retweets (>5000) of the tweet "This was days before my husband took his own life. Suicidal thoughts were there, but you'd Never know.  Fuck depression."

### Peak on 2017-10-05

```{r, wordcloud  2017-10-05, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2017-10-05")
wordcloud_suicide(dcloud)
```

Retweets of the following tweet, a response to a [tweet](https://thehill.com/homenews/administration/353763-trump-its-a-miracle-how-fast-las-vegas-police-found-the-shooter-and) by Donald Trump about how it was a "miracle" how fast the police stopped the shooter at the [Las Vegas shooting](https://en.wikipedia.org/wiki/2017_Las_Vegas_shooting)

```{r}
dp1 %>% 
  filter(main_category == "werther"& date=="2017-10-05") %>% 
  select(text) %>% slice(1)
```
### Peak on 2017-11-04

```{r, wordcloud  2017-11-04, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2017-11-04")
wordcloud_suicide(dcloud)
```

Retweets of a tweet about a girlfriend that killed herself 

```{r}
dp1 %>% 
  filter(main_category == "werther"& date=="2017-11-04") %>% 
  select(text) %>% slice(1)
```
### Peak on 2018-02-02


```{r, wordcloud 2018-02-02, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2018-02-02")
wordcloud_suicide(dcloud)
```

```{r}
dp %>% 
  filter(main_category == "werther"& date=="2018-02-02") %>% 
  filter(str_detect(text, "Fidel Castro")) %>% 
  select(text) %>% slice(1)
```

### Peak on 2018-06-07


```{r, wordcloud 2018-06-07, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2018-06-07")
wordcloud_suicide(dcloud)
```

### Peak on 2018-06-08

Anthony Bourdain

```{r, wordcloud 2018-06-08, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2018-06-08")
wordcloud_suicide(dcloud)
```

### Peak on 2017-09-09	

```{r, wordcloud 2017-04-19	, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2017-04-19")
wordcloud_suicide(dcloud)
```

### Peak on 2017-07-20

```{r, wordcloud 2017-07-20	, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2017-07-20")
wordcloud_suicide(dcloud)
```

### Peak on 2018-04-28

```{r, wordcloud 2018-04-28, cache=TRUE}
#pick the day with a peak: 
dcloud = dp1 %>% 
  filter(main_category == "werther"& date=="2018-04-28")
wordcloud_suicide(dcloud)
```

```{r}
dp %>% 
  filter(main_category == "werther"& date=="2018-04-28") %>% 
  filter(str_detect(text, "avicii")) %>% 
  select(text)
```