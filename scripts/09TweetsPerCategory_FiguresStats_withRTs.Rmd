---
title: "Tweets per Category in the ML predictions dataset (BERT) based on Crimson Hexagon query with retweets"
author: "Hannah Metzler"
date: "4th October 2021"
output: 
  pdf_document:
    df_print: kable
    keep_tex: true
url_colour: blue
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, results=T)
Sys.setlocale("LC_ALL", 'en_US.UTF-8')
options(scipen=99)
library(dplyr)
library(ggplot2)
library(viridis)
library(tidyr)
library(lubridate)
```


```{r data, include=FALSE}
#load data (origin: scripts folder, where this .Rmd file is)
load("../data_tweet_volumes/Suicide_tweets_daily_volume_per_maincategory_withRTs.R")
# load("../data_tweet_volumes/Suicide_tweets_daily_volume_aboutsuicide.R")

#our source script (but this takes a couple of minutes)
# source('06BuildPredictionsTimelineDataset.R')

#assign colours to categories:
catcols = viridis(6)
mycols = c("awareness"= catcols[1], "werther"=catcols[6], "prevention"=catcols[4], "suicidality"=catcols[5], "coping"=catcols[2], "irrelevant"=catcols[3])

#order of categories for plots
ord_mcategory= c( "suicidality", "coping", "awareness", "prevention", "werther", "irrelevant")

#rename main_category_predictions to predictions, but keep in mind about_suicide_n is also calculated based on model predictions (with the categories yes/no)
dcatperday = dcatperday %>% 
  rename(prediction = main_category_prediction) %>% 
  #category order
  mutate(prediction = factor(prediction, levels = ord_mcategory, labels = ord_mcategory))

summary(dcatperday)
```


# Dataset description

We initially downloaded tweets from crimson hexagon on April 15th 2020, including all tweets containing a suicide-related term from January 1st 2013 to April 14th 2020. We excluded terms that clearly do not refer to suicide as the act of killing oneself. The query on Crimson Hexagon was: 

country:USA AND language:en
AND (suicide OR suicidal OR "killed himself" OR "killed herself" OR "kill himself" OR "kill herself" OR "hung himself" OR "hung herself" OR "took his life" OR "took her life" OR "take his life" OR "take her life" OR "end his own life" OR "end her own life" OR "ended his own life" OR "ended her own life" OR "end his life" OR "end her life" OR "ended his life" OR "ended her life" OR "ends his life" OR "ends her life") 
AND NOT ("suicide squad" OR suicidechrist OR suicidegirl* OR suicideboy* OR suicideleopard OR suicidexjockey* OR "suicidal grind" OR bomber OR squad OR epstein OR Trump OR clinton* OR Hillary OR Biden OR sanders OR “political suicide”) 

We rehydrated these tweets a first time in April 2020, the resulting dataset included around 13.428 million tweets, of which only 9.584 million had unique ids. The machine learning models were trained on a sample of 3200 manually annotated tweets from this sample. Because our initial try to multiply each tweet by its number of retweet did not accurately mirror the daily volumes directly downloaded via Brandwatch, we set up another query with retweets, and rehydrated this tweets. This document describes this final dataset, which was used in the follow-up timeseries analysis. 

Predictions were made with the BERT base model, fine-tuned (more epochs, smaller learning rate, check with Hubert for details). 

* First included date: 2016-01-01
* Last included date: 2021-04-13

# Sample descriptives 
### Total sample size

```{r}
dcatperday %>%
  group_by(date) %>% #only first line of 6 categories (ndaytotal is the same on each line)
  slice(1) %>% 
  ungroup() %>% 
  summarise(mio_tweets = sum(ndaytotal)/1000000)
```

Total sample size 2016-2018 for paper: 

```{r}
dcatperday %>%
  filter(year %in% c(2016, 2017, 2018)) %>% 
  group_by(date) %>% #only first line of 6 categories (ndaytotal is the same on each line)
  slice(1) %>% 
  ungroup() %>% 
  summarise(mio_tweets = sum(ndaytotal))
```


### Sample per year 
```{r}
dcatperday %>%
  group_by(date) %>% #only first line of 6 categories (ndaytotal is the same on each line)
  slice(1) %>% 
  group_by(year) %>% 
  summarise(mio_tweets = sum(ndaytotal)/1000000) %>% 
  #2020 is not a full year 
  filter(year!="2021")
```

### Total number per category for paper (2016-2018)

```{r}
prop.pred.grand = as.data.frame(
  dcatperday %>%
    filter(year %in% c(2016, 2017, 2018)) %>% 
    group_by(prediction) %>%
    summarise (n = sum(ntweets)) %>%
    mutate(freq = round(n/ sum(n)*100,2)))

#CI
binomci <- function(prop.pred.grand){
  for (i in seq(1:nrow(prop.pred.grand))){
    ci =with(prop.pred.grand, binom.test(x = n[i], n =sum(n)))$conf.int
    prop.pred.grand$cilow[i] = ci[1]
    prop.pred.grand$cihigh[i] = ci[2]
  }
 return(prop.pred.grand)
}
prop.pred.grand = binomci(prop.pred.grand)

#adjust by the recall of the model: divide by the recall (because I want to increase the number if the recall is low)
prop.pred.grand$recall_bert = c(0.45, 0.69, 0.70, 0.89, 0.77, NA)
prop.pred.grand = prop.pred.grand %>% 
  mutate(n_adjusted = n/recall_bert) %>% 
  mutate(freq_adjusted = round(n_adjusted/sum(prop.pred.grand$n)*100,2))
  #correct n for category irrelevant (because dividing by recall does not work there)
prop.pred.grand[prop.pred.grand$prediction=="irrelevant", "freq_adjusted"] = 100-sum(prop.pred.grand$freq_adjusted, na.rm = T)
write.csv(file="../results/estimated_freq_per_category_retweet_dataset1.csv", prop.pred.grand)
prop.pred.grand
```

```{r}
ggplot(prop.pred.grand, aes(x=prediction, y = freq, fill=prediction)) + #dataset and variables to plot
  geom_bar(stat="identity")+
  geom_errorbar(aes(x=prediction, ymin = freq-cilow, ymax=freq+cihigh), size=1, width=0.5)+
  ggtitle("Tweets per predicted category")+
  labs(y="Proportion", x="") + #axes and title labels
  scale_fill_manual(values=c("#332288", "#88CCEE", "#44AA99", "#117733", '#999933', '#DDCC77', '#CC6677', "#882255", "#332288", "#88CCEE", "#44AA99", "#AA4499", "#999999",  "#444444"))+
  theme_bw() +  
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position="none", text=element_text(size=s))
```
Not adjusted by model recall

### Standard deviation, min and max across daily proportions per category

```{r}
prop.pred.daily = dcatperday %>%
  filter(year %in% c(2016, 2017, 2018)) %>% 
  #collapse across tweet type (original vs retweet)
  group_by(date, prediction) %>% 
  mutate(ncat = sum(ntweets)) %>% 
  slice(1) %>% #drop 2nd line
  group_by(prediction) %>% 
  
  #daily percentage
  mutate(freq = ncat/ndaytotal*100) %>% 
  #mean and sd across all days
  group_by(prediction) %>% 
  summarise(mean = round(mean(freq),2),
            sd = round(sd(freq),2), 
            min = round(min(freq), 2), 
            max = round(max(freq), 2)) %>% 
  #adjust for model recall
  mutate(mean_adjusted = round(mean/prop.pred.grand$recall_bert, 2))
#correct n for category irrelevant (because dividing by recall does not work there)
prop.pred.daily[prop.pred.daily$prediction=="irrelevant", "mean_adjusted"] = 100-sum(prop.pred.daily$mean_adjusted, na.rm = T)
prop.pred.daily
#   write.csv(file="../results/estimated_freq_per_category_retweet_dataset_sd_min_max.csv", prop.pred.daily)

ggplot(prop.pred.daily, aes(x=prediction, y = mean, fill=prediction)) + #dataset and variables to plot
  geom_bar(stat="identity")+
  # geom_errorbar(aes(x=prediction, ymin = mean_adjusted-cilow, ymax=mean_adjusted+cihigh), size=1, width=0.5)+
  ggtitle("Average across daily percentages")+
  labs(y="Proportion", x="") + #axes and title labels
  scale_fill_manual(values=c("#332288", "#88CCEE", "#44AA99", "#117733", '#999933', '#DDCC77', '#CC6677', "#882255", "#332288", "#88CCEE", "#44AA99", "#AA4499", "#999999",  "#444444"))+
  theme_bw() +  
  theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position="none", text=element_text(size=s))

```



## Total number of tweets per category by year

* Irrelevant increase over the years may simply reflect increase in tweets overall. Irrelevant includes tweets about suicide not belonging in any other category, and off-topic tweet (metaphors, jokes, euthansia, bombings,...). 
* the total volume (sum of all categories) is shown in black

```{r}
dcatperyear = dcatperday %>% 
  group_by(year, predictions) %>% 
  summarise(ntweets = sum(ntweets), 
            pr = sum(ntweets)/sum(ndaytotal)) %>% 
  group_by(year) %>% 
       mutate(ntotal = sum(ntweets)) %>% 
  #2021 is not a full year 
  filter(year!="2021") %>% 
  #million tweets
  mutate(mio_tweets = ntweets/1000000,
         mio_total = ntotal/1000000)

#plot per year
ggplot(data=dcatperyear, aes(x=year, y=ntweets))+
  geom_point(aes(colour = predictions))+ geom_line(aes(colour = predictions))+
  geom_point(aes(x=year, y=ntotal))+  geom_line(aes(x=year, y=ntotal))+
  #show total tweets with suicide terms per year (sum of all categories)
  # geom_line(data=dtotperyear, aes(x=year, y=ndaytotal))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  scale_x_continuous(n.breaks=7)
```

```{r}
ggplot(data=filter(dcatperyear, predictions!="irrelevant"), aes(x=year, y=mio_tweets, colour = predictions))+
  geom_point()+ geom_line()+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  scale_x_continuous(n.breaks=7)+
  ggtitle(" Same plot without the category Irrelevant")
  
```

* also shows the general increase for Werther tweets. 
* the increase exists since 2017, but is much smaller, for coping and suicidality tweets

The percentage of tweets per category decreased for irrelevant, and increased for the other categories. 

```{r}
#plot per year
ggplot(data=dcatperyear, aes(x=year, y=pr))+
  geom_point(aes(colour = predictions))+ geom_line(aes(colour = predictions))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  scale_x_continuous(n.breaks=7)+ggtitle("Percentage of tweets per category in total number of tweets")
```


# Tweets per month in one example year, here 2017

To compare with the volume on Brandwatch and in each category. 

* On Brandwatch, October 2017 has 170.000 without retweets, 380.000 with all tweets. 185.000 in our dataset lies in that range. 
* On Brandwatch, January 2017 has 160.000 without retweets, 555.000 with all tweets. 193.000 in our dataset lies in that range. 

```{r}
#per month in 2017
dcatperday %>% 
  mutate(month = floor_date(date, unit="month")) %>% 
  group_by(month) %>% 
  summarise(ntweets = sum(ntweets)) %>% 
  filter(month>as.Date("2016-12-31")) %>% 
  slice(1:12)
```


# Monthly pattern? Total number of tweets per month (averaged across 7 years)

```{r}
dcatpermonth = dcatperday %>% 
  filter(date < as.Date("2020-04-01")) %>% #no full month of data
  group_by(month, predictions) %>% 
  summarise(ntweets = sum(ntweets)) %>% 
  #thousand tweets
  mutate(thousand_tweets = ntweets/1000) %>% ungroup()
#plot per month
ggplot(data=dcatpermonth, aes(x=month, y=ntweets, colour = predictions))+
  geom_point()+ geom_line(aes(group=predictions))+
  theme_bw()+
  scale_colour_manual(values=mycols)

```

* Open question: Are the peaks in certain categories driven by outlier months in some years, or general patterns?
    - September: world suicide prevention day is likely a general pattern
    - Werther/Prevention ups and downs are probably influenced by celebrity suicides in one or some of the years.

# Monthly timeline across years (January 2015 to March 2020)

```{r}
#monthly timeseries
dtsmonthly = dcatperday %>% 
  #filter(date < as.Date("2020-04-01")) %>% #no full month of data
  group_by(year, month, predictions) %>% 
  summarise(ntweets = sum(ntweets)) %>% 
  #thousand tweets
  mutate(thousand_tweets = ntweets/1000) %>% ungroup() %>% 
#add median per month 
  group_by(month, predictions) %>% 
  mutate(ntweets_median = median(ntweets))

#plot per month
ggplot(data=dtsmonthly)+
  geom_point(aes(x=month, y=ntweets, colour = predictions))+ 
  geom_line(aes(x=month, y=ntweets_median, group=predictions, colour=predictions))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("One dot per year, median as a line")
```


More detailed picture (without irrelevant category):

```{r}
ggplot(data=filter(dtsmonthly, predictions!="irrelevant"))+
  geom_point(aes(x=month, y=ntweets, colour = predictions))+ 
  geom_line(aes(x=month, y=ntweets_median, group=predictions, colour=predictions))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("One dot per year, median as a line (without irrelevant)")
```



```{r}
ggplot(data=filter(dtsmonthly, predictions!="irrelevant"))+
  # geom_violin(aes(x=month, y=ntweets, colour = predictions))+ 
  geom_point(aes(x=month, y=ntweets, colour = predictions))+ 
  geom_line(aes(x=month, y=ntweets_median, group=predictions, colour=predictions))+
  facet_wrap(~predictions)+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  theme(axis.text.x=element_text(angle=60, hjust=1), axis.title.x=element_blank(), legend.position="none")+
  ggtitle("One dot per year, median as a line")
```

* September prevention is systematically higher (world suicide prevention day). Maybe also slightly higher around Christmas. 
* The Werther outlier in June could be Spade and Bourdain in 2018?

# Entire timeline of daily tweets per category

```{r}
ggplot(data=dcatperday, aes(x=date, y=ntweets, colour = predictions))+
  geom_line()+
  theme_bw()+
  scale_colour_manual(values=mycols)
```

```{r, coping suicidality daily}
ggplot(data=filter(dcatperday, predictions=="coping" | predictions=="suicidality"), 
                   aes(x=date, y=ntweets, colour = predictions))+
    geom_line()+ facet_wrap(~predictions, ncol=1)+
  theme_bw()+theme(legend.position="none")+
  scale_colour_manual(values=mycols)+
  ggtitle("Only coping and suicidality")
```

<!-- On which days are these highest peaks? Peaks above 2000 daily tweets per coping/suicidality: -->

<!-- ```{r} -->
<!-- dcatperday %>%  -->
<!--  filter(predictions=="coping" | predictions=="suicidality") %>%  -->
<!--   filter(ntweets>2000) %>%  -->
<!--   select(predictions, date, ntweets) %>%  -->
<!--   arrange(predictions) -->
<!-- ``` -->

## Awareness, Prevention, Werther

```{r}
ggplot(data=filter(dcatperday, predictions=="awareness" | predictions=="prevention"  | predictions=="werther"),
                   aes(x=date, y=ntweets, colour = predictions))+
  geom_line()+ facet_wrap(~predictions, ncol=1)+
  theme_bw()+theme(legend.position="none")+
  scale_colour_manual(values=mycols)
```

<!-- * Large awareness peaks above 6000 daily tweets: -->

<!-- ```{r} -->
<!-- dcatperday %>%  -->
<!--   filter(predictions=="awareness") %>%  -->
<!--   filter(ntweets>6000) %>%  -->
<!--   select(date, ntweets) -->
<!-- ``` -->

<!-- * 11 large prevention peaks, above 6000 daily tweets: -->
<!--     - around September 10, around Christmas, beginning of pandemic, beginning of September 2020 (covid?) -->

<!-- ```{r} -->
<!-- dcatperday %>%  -->
<!--   filter(predictions=="prevention") %>%  -->
<!--   filter(ntweets>6000) %>%  -->
<!--   select(date, ntweets) -->
<!-- ``` -->


<!-- * 5  large Werther peaks, above 12000 daily tweets (there are several other important but smaller peaks) -->

<!-- ```{r} -->
<!-- dcatperday %>%  -->
<!--   filter(predictions=="werther") %>%  -->
<!--   filter(ntweets>6000) %>%  -->
<!--   select(date, ntweets) -->
<!-- ``` -->


# Weekly pattern? Total number of tweets per weekday (averaged across 7 years)

```{r}
dcatperwday = dcatperday %>% 
  group_by(weekday, predictions) %>% 
  summarise(median_tweets = median(ntweets)) %>% 
  #thousand tweets
  mutate(thousand_tweets = median_tweets/1000) %>% ungroup()
#plot per week day
ggplot(data=dcatperwday, aes(x=weekday, y=median_tweets, colour = predictions))+
  geom_point()+ geom_line(aes(group=predictions))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("Median tweets per weekday")
ggplot(data=filter(dcatperwday, predictions !="irrelevant"), aes(x=weekday, y=median_tweets, colour = predictions))+
  geom_point()+ geom_line(aes(group=predictions))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("Median tweets per weekday (without irrelevant category)")
ggplot(data=filter(dcatperwday, predictions =="coping" | predictions == "suicidality"), aes(x=weekday, y=median_tweets, colour = predictions))+
  geom_point()+ geom_line(aes(group=predictions))+
  theme_bw()+
  scale_colour_manual(values=mycols)+
  ggtitle("Median tweets per weekday: suicidality and coping category)")
```

